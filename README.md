[![Shell](https://img.shields.io/badge/Shell-121011?style=flat&logo=gnu-bash&logoColor=white)](https://www.gnu.org/software/bash/)
[![Python](https://img.shields.io/badge/Python-3776AB?style=flat&logo=python&logoColor=white)](https://www.python.org/)
[![Hadoop Streaming](https://img.shields.io/badge/Hadoop%20Streaming-66CCFF?style=flat&logo=apache-hadoop&logoColor=white)](https://hadoop.apache.org/docs/current/hadoop-streaming/HadoopStreaming.html)
[![Apache Spark](https://img.shields.io/badge/Apache%20Spark-E25A1C?style=flat&logo=apache-spark&logoColor=white)](https://spark.apache.org/)

# Wikipedia Machine Learning Article Evolution Analysis

This project analyzes the historical evolution of the Wikipedia "Machine learning" article using a combination of shell scripts, Hadoop streaming, and Apache Spark. It extracts article revisions, parses content, and computes co-occurrence and n-gram statistics over time.

---

## ğŸ“ Project Structure

```
.
â”œâ”€â”€ data                        # Contains articles about ml crawled from Wikipedia
â”œâ”€â”€ hadoop
â”‚   â”œâ”€â”€ scripts
â”‚   â”‚   â”œâ”€â”€ hadoop_setup.sh     # Download & install Hadoop
â”‚   â”‚   â”œâ”€â”€ upload_to_hdfs.sh   # Upload articles' text file to Hadoop
â”‚   â”‚   â”œâ”€â”€ run_bigram.sh       # Run mapper&reducer to analyse bigram
â”‚   â”‚   â””â”€â”€ run_cooccur.sh       # Run mapper&reducer to analyse co-occurrence
â”‚   â””â”€â”€ streaming
â”‚   â”‚   â”œâ”€â”€ mapper_cooccur.py
â”‚   â”‚   â”œâ”€â”€ reducer_cooccur.py
â”‚   â”‚   â”œâ”€â”€ mapper_bigram.py
â”‚   â”‚   â”œâ”€â”€ reducer_bigram.py
â”‚   â”‚   â””â”€â”€ combiner_bigram.py
â”œâ”€â”€ README.md
â”œâ”€â”€ scripts
â”‚   â”œâ”€â”€ data_collection.py
â”‚   â”œâ”€â”€ generate_article_ids.py # Choose 1 article in 1 month since 2020
â”‚   â”œâ”€â”€ download.sh             # Get Wikipedia html based on article ids
â”‚   â””â”€â”€ parse_article.sh        # Parse raw ml html to text and clean
â”œâ”€â”€ spark
â”‚   â”œâ”€â”€ scripts
â”‚   â”‚   â””â”€â”€ spark_setup.sh      # Install & configure spark
â”‚   â””â”€â”€ streaming
â”‚   â”‚   â”œâ”€â”€ map_reducer_coocur.py
â”‚   â”‚   â””â”€â”€ map_reducer_ngram.py
â”œâ”€â”€ analysis
â”‚   â”œâ”€â”€ heatmap_utils.py        # Load data from csv file
â”‚   â”œâ”€â”€ heatmap_plot.py         # Plot heatmap of co-occurrence pairs
â”‚   â””â”€â”€ trend_plot.py           # Plot trend of top 2 co-occurrence pairs
â”œâ”€â”€ artifacts                   # Store generated csv files and images
â””â”€â”€ stopwords.txt
```

---

## âš™ï¸ Setup Instructions

1. Clone the repository and navigate to the project root.

2. Prepare data <br>    
    Generate article revision IDs:
   ```bash
   python scripts/generate_article_ids.py
   ```

    Download and parse Wikipedia articles:
   ```bash
   python scripts/data_collection.py
   ```

3. Install dependencies:
   ```bash
   pip install -r requirements.txt
   ```

4. Download and extract Spark (for Spark part) :
   ```bash
   bash spark/scripts/spark_setup.sh
   ```

5. Download and set up Hadoop (for Hadoop part):
   ```bash
   bash hadoop/scripts/hadoop_setup.sh
   ```

6. Upload article files to hdfs (for Hadoop part):
   ```bash
   bash upload_to_hdfs.sh
   ```

---

## ğŸš€ Analysis Scripts

### Co-occurrence Analysis (Spark)

Compute word pair co-occurrence frequencies per year:

```bash
spark-submit spark/streaming/map_reducer_coocur.py
```

### N-gram Analysis (Spark)

Compute n-gram frequencies per year (change the argument --n):

```bash
spark-submit spark/streaming/map_reducer_ngram.py --n 3
```

---

## ğŸš€ Analysis Scripts

### Co-occurrence Analysis (Hadoop)

Compute word pair co-occurrence frequencies:

```bash
bash hadoop/scripts/run_coocur.py
```

### 2-gram Analysis (Hadoop)

Compute 2-gram frequencies:

```bash
bash hadoop/scripts/run_bigram.py
```

---

## ğŸ“ˆ Usage

This project explores the evolution of Wikipedia language by analyzing co-occurrence patterns and n-gram frequencies across different time periods. By examining how word pairs and sequences change over time, we gain insights into linguistic trends and topic shifts in Wikipedia edits.

ğŸ”¥ Plot the heatmap of co-occurrence pairs:
```bash
python analysis/heatmap_plot.py
```
The output will be like this:

![Heatmap of Co-occurrance throught all years](artifacts/visuals/cooccur_heatmap_total.png)

ğŸ”¥ Plot the trend of top 3 co-ocur pairs:
```bash
python analysis/trend_plot.py
```
The output will be like this:

![Heatmap of Co-occurrance throught all years](artifacts/visuals/cooccur_trends.png)

It provides a data-driven lens into:

- The evolution of terminology in machine learning

- The rise and fall of specific concepts or techniques

- The linguistic dynamics of Wikipedia as a living, collaborative corpus

These insights offering researchers and language analysts a way to explore how knowledge is framed and communicated over time. 


For example, the heatmap visualizes the frequency of word pairs (co-occurrence) aggregated across selected Wikipedia snapshots. Each cell represents how often a specific word pair appears, allowing us to identify dominant linguistic patterns and emerging terminology.




