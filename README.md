[![Shell](https://img.shields.io/badge/Shell-121011?style=flat&logo=gnu-bash&logoColor=white)](https://www.gnu.org/software/bash/)
[![Python](https://img.shields.io/badge/Python-3776AB?style=flat&logo=python&logoColor=white)](https://www.python.org/)
[![Hadoop Streaming](https://img.shields.io/badge/Hadoop%20Streaming-66CCFF?style=flat&logo=apache-hadoop&logoColor=white)](https://hadoop.apache.org/docs/current/hadoop-streaming/HadoopStreaming.html)
[![Apache Spark](https://img.shields.io/badge/Apache%20Spark-E25A1C?style=flat&logo=apache-spark&logoColor=white)](https://spark.apache.org/)

# Wikipedia Machine Learning Article Evolution Analysis

This project analyzes the historical evolution of the Wikipedia "Machine learning" article using a combination of shell scripts, Hadoop streaming, and Apache Spark. It extracts article revisions, parses content, and computes co-occurrence and n-gram statistics over time.

---

## ğŸ“ Project Structure

```
.
â”œâ”€â”€ data                        # Contains articles about ml crawled from Wikipedia
â”œâ”€â”€ hadoop
â”‚   â”œâ”€â”€ scripts
â”‚   â”‚   â”œâ”€â”€ hadoop_setup.sh     # Download & install Hadoop
â”‚   â”‚   â”œâ”€â”€ upload_to_hdfs.sh   # Upload articles' text file to Hadoop
â”‚   â”‚   â”œâ”€â”€ run_bigram.sh       # Run mapper&reducer to analyse bigram
â”‚   â”‚   â”œâ”€â”€ run_cooccur.sh       # Run mapper&reducer to analyse co-occurrence
â”‚   â””â”€â”€ streaming
â”‚   â”‚   â”œâ”€â”€ mapper_cooccur.py
â”‚   â”‚   â”œâ”€â”€ reducer_cooccur.py
â”‚   â”‚   â”œâ”€â”€ mapper_bigram.py
â”‚   â”‚   â”œâ”€â”€ reducer_bigram.py
â”‚   â”‚   â”œâ”€â”€ combiner_bigram.py
â”œâ”€â”€ README.md
â”œâ”€â”€ scripts
â”‚   â”œâ”€â”€ data_collection.py
â”‚   â”œâ”€â”€ generate_article_ids.py # Choose 1 article in 1 month since 2020
â”‚   â”œâ”€â”€ download.sh             # Get Wikipedia html based on article ids
â”‚   â””â”€â”€ parse_article.sh        # Parse raw ml html to text and clean
â”œâ”€â”€ spark
â”‚   â”œâ”€â”€ scripts
â”‚   â”‚   â”œâ”€â”€ spark_setup.sh      # install & configure spark
â”‚   â””â”€â”€ streaming
â”‚   â”‚   â”œâ”€â”€ map_reducer_coocur.py
â”‚   â”‚   â”œâ”€â”€ map_reducer_ngram.py
â””â”€â”€ stopwords.txt
```

---

## âš™ï¸ Setup Instructions

1. Clone the repository and navigate to the project root.

2. Prepare data <br>    
    Generate article revision IDs:
   ```bash
   python scripts/generate_article_ids.py
   ```

    Download and parse Wikipedia articles:
   ```bash
   python scripts/data_collection.py
   ```

3. Install dependencies:
   ```bash
   pip install findspark
   ```

4. Download and extract Spark :
   ```bash
   bash spark/scripts/spark_setup.sh
   ```

5. Download and set up Hadoop:
   ```bash
   bash hadoop/scripts/hadoop_setup.sh
   ```

6. Upload article files to hdfs:
   ```bash
   bash upload_to_hdfs.sh
   ```

---

## ğŸš€ Analysis Scripts

### Co-occurrence Analysis (Spark)

Compute word pair co-occurrence frequencies per year:

```bash
spark-submit spark/streaming/map_reducer_coocur.py
```

### N-gram Analysis (Spark)

Compute n-gram frequencies per year (change the argument --n):

```bash
spark-submit spark/streaming/map_reducer_ngram.py --n 3
```

---

## ğŸ“Œ Notes

- All parsed articles are stored in `data/wikipedia-ml/` with filenames like:
  ```
  article_Machine learning-Wikipedia_2016_7_14.txt
  ```

- Stopwords are loaded from `stopwords.txt` and broadcasted in Spark jobs.

- Both Spark scripts include robust text cleaning and filtering logic:
  - Remove punctuation, digits, HTML tags, and short or meaningless tokens.
  - Only retain alphabetic words with length â‰¥ 3 not in the stopword list.

---

## ğŸ“ˆ Output

Each analysis script prints the top 10 results per year to the console. You can modify the scripts to save results to CSV or visualize trends.
